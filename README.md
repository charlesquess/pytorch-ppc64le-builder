# PyTorch 2.5.1 Builder for PowerPC (ppc64le)
# PyTorch 2.5.1 PowerPC (ppc64le) ä¸€é”®ç¼–è¯‘è„šæœ¬

This repository provides a reliable shell script to build PyTorch from source on IBM PowerPC architecture. It automatically patches known issues regarding GCC 12 compatibility, Gloo/AVX conflicts, and FlatBuffers version mismatches.

æœ¬ä»“åº“æä¾›äº†ä¸€ä¸ªåœ¨ IBM PowerPC æ¶æ„ä¸Šä»æºç ç¼–è¯‘ PyTorch çš„å¯é è„šæœ¬ã€‚å®ƒè‡ªåŠ¨ä¿®è¡¥äº† GCC 12 å…¼å®¹æ€§ã€Gloo/AVX æŒ‡ä»¤é›†å†²çªä»¥åŠ FlatBuffers ç‰ˆæœ¬ä¸åŒ¹é…ç­‰å·²çŸ¥é—®é¢˜ã€‚

## ğŸ–¥ï¸ Build Environment / ç¼–è¯‘ç¯å¢ƒ

Tested successfully under the following configuration:
è¯¥è„šæœ¬å·²åœ¨ä»¥ä¸‹é…ç½®ä¸­éªŒè¯é€šè¿‡ï¼š

| Component (ç»„ä»¶) | Version / Details (ç‰ˆæœ¬/è¯¦æƒ…) | Note (å¤‡æ³¨) |
| :--- | :--- | :--- |
| **PyTorch Source** | `v2.5.1` (Alpha/Source) | Compiled from source |
| **Architecture** | `ppc64le` (IBM Power) | Little Endian |
| **OS** | Linux (Debian/Ubuntu/RHEL) | |
| **Compiler** | **GCC 12+** | Requires patches (Included) |
| **CUDA Toolkit** | 12.1 | *Verify with `nvcc --version`* |
| **Python** | 3.9 | Conda environment recommended |
| **GPU Hardware** | NVIDIA Tesla V100 (SXM2) | Supports multi-GPU (NCCL) |
| **Build System** | Ninja + CMake | |

---

## ğŸš€ Key Features / ä¸»è¦åŠŸèƒ½

It automatically applies patches for the following known issues:
è„šæœ¬è‡ªåŠ¨ä¿®å¤äº†ä»¥ä¸‹å·²çŸ¥çš„é«˜éš¾åº¦ç¼–è¯‘é—®é¢˜ï¼š

1.  **Gloo AVX Error**: Disables AVX instructions in `allreduce_shm.cc` which are incompatible with PowerPC.
    * *ä¿®å¤ Gloo åº“å¼ºåˆ¶è°ƒç”¨ Intel AVX æŒ‡ä»¤é›†å¯¼è‡´åœ¨ PowerPC ä¸ŠæŠ¥é”™çš„é—®é¢˜ã€‚*
2.  **GCC 12 `constinit` Compatibility**: Fixes `Protobuf` and `ONNX` build failures caused by C++20 strict checks.
    * *ä¿®å¤ GCC 12 ä¸¥æ ¼æ£€æŸ¥ `constinit` å…³é”®å­—å¯¼è‡´ Protobuf/ONNX ç¼–è¯‘å¤±è´¥çš„é—®é¢˜ã€‚*
3.  **FlatBuffers Version Mismatch**: Bypasses the strict version check (`static_assert`) between PyTorch source (v2.5) and system/conda headers.
    * *æš´åŠ›è§£å†³ FlatBuffers ç‰ˆæœ¬å†²çªï¼ˆå¦‚ v3 vs v26ï¼‰ï¼Œå±è”½æ‰€æœ‰ç‰ˆæœ¬æ£€æŸ¥æ–­è¨€ã€‚*
4.  **Kineto/Profiler Issues**: Rewrites `kineto_shim.cpp` to fix missing symbols (`profilerStep`, `addMetadataJson`) and syntax errors.
    * *é‡å†™ Kineto Shim æ–‡ä»¶ï¼Œè¡¥å…¨ç¼ºå¤±çš„ç¬¦å·é“¾æ¥ï¼Œä¿®å¤ switch è¯­æ³•é”™è¯¯ã€‚*
5.  **ProcessGroupGloo Type Mismatch**: Fixes pointer/reference mismatch in `ProcessGroupGloo.cpp`.
    * *ä¿®å¤ Gloo è¿›ç¨‹ç»„ä¸­çš„æŒ‡é’ˆç±»å‹ä¸åŒ¹é…é”™è¯¯ã€‚*

---

## ğŸ“‹ Prerequisites / ç¯å¢ƒè¦æ±‚

* **OS**: Linux (Debian/Ubuntu/CentOS/RHEL) on ppc64le
* **Architecture**: IBM Power (ppc64le)
* **GPU**: NVIDIA Tesla V100/A100 (Verified on V100)
* **Environment**: Conda (Python 3.9 recommended)
* **Compiler**: GCC 12+

---

## ğŸ› ï¸ Usage / ä½¿ç”¨æ–¹æ³•

### 1. Download PyTorch Source / ä¸‹è½½æºç 
Clone the PyTorch repository (recursive is important):
é¦–å…ˆä¸‹è½½ PyTorch æºç ï¼ˆæ³¨æ„å¿…é¡»åŒ…å«å­æ¨¡å—ï¼‰ï¼š

```bash
git clone --recursive --branch v2.5.1 [https://github.com/pytorch/pytorch.git](https://github.com/pytorch/pytorch.git)
cd pytorch
```
### 2. Download the Script / ä¸‹è½½è„šæœ¬
Download `build_pytorch_ppc64le.sh` and place it **inside the root of your PyTorch source directory**.
ä¸‹è½½ `build_pytorch_ppc64le.sh` å¹¶å°†å…¶**æ”¾å…¥ PyTorch æºç çš„æ ¹ç›®å½•ä¸­**ã€‚

> **Why?** The script relies on relative paths to apply patches correctly.
> **ä¸ºä»€ä¹ˆï¼Ÿ** è„šæœ¬ä¾èµ–ç›¸å¯¹è·¯å¾„æ¥ç²¾å‡†åº”ç”¨è¡¥ä¸ï¼Œæ”¾å…¥æ ¹ç›®å½•æœ€ä¸ºç¨³å¦¥ã€‚

Structure should look like this:
ç›®å½•ç»“æ„åº”è¯¥åƒè¿™æ ·ï¼š
```text
pytorch/
â”œâ”€â”€ .git/
â”œâ”€â”€ torch/
â”œâ”€â”€ third_party/
â”œâ”€â”€ setup.py
â””â”€â”€ build_pytorch_ppc64le.sh  <-- Put it here (æ”¾è¿™é‡Œ)
```
### 3. Run Build / å¼€å§‹ç¼–è¯‘
Make sure you are in the conda environment. ç¡®ä¿ä½ å·²ç»æ¿€æ´»äº† Conda ç¯å¢ƒã€‚
```bash
cd pytorch
chmod +x build_pytorch_ppc64le.sh
./build_pytorch_ppc64le.sh
```
### 4. Install / å®‰è£…
After a successful build, the script will generate a .whl file in the dist/ folder and try to install it automatically. ç¼–è¯‘æˆåŠŸåï¼Œè„šæœ¬ä¼šåœ¨ dist/ ç›®å½•ä¸‹ç”Ÿæˆ .whl å®‰è£…åŒ…å¹¶å°è¯•è‡ªåŠ¨å®‰è£…ã€‚

You can also install it manually: ä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨å®‰è£…ï¼š

```bash
pip install dist/torch-2.5.0*.whl
```
## ğŸ” Verification / éªŒè¯
Run the following python code to verify CUDA support: è¿è¡Œä»¥ä¸‹ä»£ç éªŒè¯ CUDA æ”¯æŒï¼š

```python
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"Device Count: {torch.cuda.device_count()}")

# Test Tensor Calculation
x = torch.rand(5, 3).cuda()
print(x)
```
## Notes / æ³¨æ„äº‹é¡¹
* Environment Variables: The script automatically sets CMAKE_PREFIX_PATH based on your current Conda environment.ç¯å¢ƒå˜é‡: è„šæœ¬ä¼šæ ¹æ®å½“å‰çš„ Conda ç¯å¢ƒè‡ªåŠ¨è®¾ç½® CMAKE_PREFIX_PATHã€‚
* Windows to Linux: If you downloaded the source on Windows and transferred it to Linux, run this script directly. It includes a fix for CRLF line endings and file permissions.æ–‡ä»¶æ ¼å¼: å¦‚æœä½ çš„æºç æ˜¯ä» Windows ä¼ è¾“è¿‡æ¥çš„ï¼Œè„šæœ¬å†…ç½®äº† CRLF è½¬ LF çš„ä¿®å¤åŠŸèƒ½ï¼Œç›´æ¥è¿è¡Œå³å¯ã€‚
* FlatBuffers: The script might rename your Conda's include/flatbuffers to flatbuffers.bak temporarily to avoid conflicts. It restores it after compilation (if successful).å…³äº FlatBuffers: ä¸ºäº†é˜²æ­¢å†²çªï¼Œè„šæœ¬å¯èƒ½ä¼šä¸´æ—¶å°† Conda ç¯å¢ƒä¸­çš„ flatbuffers å¤´æ–‡ä»¶ç›®å½•é‡å‘½åä¸º .bakï¼Œç¼–è¯‘ç»“æŸåä¼šå°è¯•æ¢å¤ã€‚

## Contributing / è´¡çŒ®
If you find this script helpful or encounter new issues on other PowerPC machines, feel free to open an issue or PR. å¦‚æœä½ è§‰å¾—è¿™ä¸ªè„šæœ¬æœ‰å¸®åŠ©ï¼Œæˆ–è€…åœ¨å…¶ä»– PowerPC æœºå™¨ä¸Šé‡åˆ°äº†æ–°é—®é¢˜ï¼Œæ¬¢è¿æäº¤ Issue æˆ– PRã€‚